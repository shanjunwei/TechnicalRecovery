{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于 统计方法的 无词典词语提取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为公司使用基于词典的机械中文分词方法，需要一个完备的词典才能有好的效果。而关键词提取的效果又依赖于中文分词效果的好坏。所以开始的初衷是找出一些原始词典里没有的词，来改善中文分词的效果。后面做着做着，似乎词语提取的办法也可以用来做中文分词，只不过评价指标上当然要差很多。\n",
    "\n",
    "\n",
    "###  1.尝试过的方法\n",
    "\n",
    ">  1. 基于词共现的词语挖掘方法。 参考论文:傅赛香, 袁鼎荣, 黄柏雄, et al. 基于统计的无词典分词方法[J]. 广西科学院学报, 2002, 18(4):252-255.\n",
    "\n",
    "这种方法采用n-gram 滑动取词并且统计频率,然后通过置信度等手段进行过滤筛选，从而得到我们需要的词。但是有的高频词片并非是一个词，如  “这一”、“然不”等。通过去除停用词再滑动统计的规则手段虽然得出的词这些明显看上去不是词的没怎么出现了，但是也会导致一些原来是词的被忽略，如 停用词 “的”可能存在 “的士”这样的词。而且这种改进规则的分词做法带来的准确率提升不明显，这种做法收益达不到预期。\n",
    "\n",
    "> 2.基于左右邻字信息熵和互信息的方法。  [互联网时代的社会语言学：基于SNS的文本数据挖掘](http://www.matrix67.com/blog/archives/5044)\n",
    "\n",
    "这种做法根据上面的博客介绍似乎是比较可行的办法，而且也不用做停用词处理。有不少论文和博客都指向这种做法，特别是比较有影响力的开源项目 HanLP也用这种方法来做词语提取并且开源，可见这种做法有较好的效果，通过人民日报的标准分词数据评测得出达到80% 的准确率，实际使用的时候再通过一些优化可以达到更好的实用化效果。\n",
    "\n",
    "## 2. 基础的技术和算法\n",
    "\n",
    "基于左右邻字信息熵和互信息的抽词方法主要基于的一些技术点和算法有：\n",
    "\n",
    ">  信息论: 信息熵、互信息\n",
    ">  nGram 滑动窗口取词 、词频统计\n",
    ">  双数组字典树\n",
    ">  常用的字符串处理方法 包括正则表达式\n",
    ">  阻塞队列 大数据存储   多线程或者多进程分治法\n",
    "\n",
    "## 3.抽词实战步骤\n",
    "\n",
    "本次抽词从单词内部凝聚度和外部自由度角度进行候选字串的筛选，计算候选词串的互信息(指示了候选字串的内部凝聚度)，计算候选字串的左右邻信息熵(指示了候选字串的外部自由度)，通过互信息及 信息熵阈值过滤掉明显不符合要求的候选词串。在剩下的候选词串中通过归一化的手段得到他们的分数，最后通过分数从大到小排序，得出最终的抽词结果。\n",
    "\n",
    "具体的抽词步骤：\n",
    "\n",
    "##### 1.对待抽词文本进行预处理(待处理文本越大效果越好，因为本文采用的是基于统计的方法)，去掉非中文字符，留下候选字片段。\n",
    "\n",
    "比如原句子：“大众汽车CEO文德森12日宣布，未来五年大众汽车将向北美市场投资70亿美元\n",
    "，力争使包括奥迪在内的品牌到2018年实现年产量100万辆的目标。”，处理完后\n",
    ">“大众汽车”，\n",
    "“文德森”，\n",
    "“日宣布”，\n",
    "“未来五年大众汽车将向北美市场投资”，\n",
    "“亿美元”，\n",
    "“力争使包括奥迪在内的品牌到”，\n",
    "“年实现年产量”，\n",
    "“万辆的目标”。\n",
    "\n",
    "得到如上所示的八个候选短句。\n",
    "\n",
    "##### 2.对候选短句以nGram 方式进行FMM滑动取词，并统计候选词的词频\n",
    "\n",
    "```\n",
    "// 切分词  FMM 算法\n",
    "    public static void FMMSegment(String text, boolean countWordFrequency) {\n",
    "        // 额外统计单个字的词频\n",
    "        wordCountSingleWord(text);\n",
    "        if (text.length() == 1) {\n",
    "            return;\n",
    "        }\n",
    "        int temp_max_len = Math.min(text.length() + 1, MAX_WORD_LEN);\n",
    "        int p = 0;\n",
    "        while (p < text.length()) {\n",
    "            int q = 1;\n",
    "            while (q < temp_max_len) {  // 控制取词的长度\n",
    "                if (q == 1) {\n",
    "                    q++;\n",
    "                    continue;  // 长度为1略过,单个汉字不具有分词意义\n",
    "                }\n",
    "                // 取词串  p --> p+q\n",
    "                if (p + q > text.length()) {\n",
    "                    break;\n",
    "                }\n",
    "                String strChar = text.substring(p, p + q);\n",
    "                // 统计词串的词频\n",
    "                if (countWordFrequency) {\n",
    "                    if (wcMap.containsKey(strChar)) {\n",
    "                        wcMap.put(strChar, wcMap.get(strChar) + 1);\n",
    "                    } else {\n",
    "                        wcMap.put(strChar, 1);\n",
    "                    }\n",
    "                }\n",
    "                q++;\n",
    "            }\n",
    "            p++;\n",
    "        }\n",
    "    }\n",
    "```\n",
    "\n",
    "如候选短句：**未来五年大众汽车将向北美市场投资**\n",
    "通过FMM 算法取词。设最大取词长度为5，得到以下54个候选词\n",
    "\n",
    ">[未来, 未来五, 未来五年, 未来五年大, 来五, 来五年, 来五年大, 来五年大众, 五年, 五年大, 五年大众, 五年大众汽, 年大, 年大众, 年大众汽, 年大众汽车, 大众, 大众汽, 大众汽车, 大众汽车将, 众汽, 众汽车, 众汽车将, 众汽车将向, 汽车, 汽车将, 汽车将向, 汽车将向北, 车将, 车将向, 车将向北, 车将向北美, 将向, 将向北, 将向北美, 将向北美市, 向北, 向北美, 向北美市, 向北美市场, 北美, 北美市, 北美市场, 北美市场投, 美市, 美市场, 美市场投, 美市场投资, 市场, 市场投, 市场投资, 场投, 场投资, 投资]\n",
    "\n",
    "##### 3. 统计计算每个候选词串的互信息，左邻字信息熵，右邻字信息熵\n",
    "\n",
    "计算信息熵主要用到的数据结构就是双数组字典树，通过字典树取到以当前字为前缀的一批词。为使得信息熵具有可对比性，当前候选词扩展一个字的算信息熵累计和，超过一个字的不算。算左邻字信息熵时，将候选字符串逆置是一个小小技巧。\n",
    "\n",
    "```\n",
    "/**\n",
    "     * 添加所有切分词  计算互信息 信息熵等统计量\n",
    "     */\n",
    "    public void addAllSegAndCompute(Map<String, Integer> wcMap) {\n",
    "        TreeMap<String, Integer> rightTreeMap = new TreeMap();\n",
    "        TreeMap<String, Integer> leftTreeMap = new TreeMap();\n",
    "        for (Map.Entry<String, Integer> entry : wcMap.entrySet()) {\n",
    "            rightTreeMap.put(entry.getKey(), entry.getValue());     // 右前缀字典树\n",
    "            leftTreeMap.put(HanUtils.reverseString(entry.getKey()), entry.getValue());  // 左前缀字典树\n",
    "            totalCount = totalCount + entry.getValue(); // 计算总词频\n",
    "        }\n",
    "        trieLeft.build(leftTreeMap);\n",
    "        trieRight.build(rightTreeMap);\n",
    "\n",
    "        for (String seg : wcMap.keySet()) {\n",
    "            // 1. 计算信息熵\n",
    "            float rightEntropy = computeRightEntropy(seg);\n",
    "            maxRE = Math.max(maxRE, rightEntropy);  // 求最大右信息熵   //totalRE = totalRE + rightEntropy;\n",
    "            float leftEntropy = computeLeftEntropy(seg);\n",
    "            maxLE = Math.max(maxLE, leftEntropy);  // 求最大左信息熵    // totalLE = totalLE + leftEntropy;\n",
    "            // 2. 计算互信息\n",
    "            float mi = computeMutualInformation(seg);\n",
    "            maxMI = Math.max(maxMI, mi);   // 计算最大互信息  //totalMI = totalMI + mi;\n",
    "            Term term = new Term(seg, wcMap.get(seg), mi, leftEntropy, rightEntropy);  // 这里没办法算最后得分\n",
    "            // 将map存入redis中\n",
    "            /**********************  redis存取 **************************/\n",
    "            redis.hmset(seg, term.convertToMap());\n",
    "            /**********************  redis存取 **************************/\n",
    "        }\n",
    "        wcMap.clear();   // 释放无用的内存\n",
    "        Term max_term = new Term(MAX_KEY, 0, maxMI, maxLE, maxRE);\n",
    "        redis.hmset(MAX_KEY, max_term.convertToMap());    // 保存最大值\n",
    "        System.out.println(\"统计量计算总耗时: \" + (System.currentTimeMillis() - t1) + \"ms\");\n",
    "    }\n",
    "```\n",
    "\n",
    "信息熵计算：\n",
    "```\n",
    "/**\n",
    "     * 计算左邻熵 \n",
    "     */\n",
    "    public float computeLeftEntropy(String prefix) {\n",
    "        Set<Map.Entry<String, Integer>> entrySet = trieLeft.prefixSearch(HanUtils.reverseString(prefix));\n",
    "        return computeEntropy(entrySet, prefix);\n",
    "    }\n",
    "\n",
    "    /**\n",
    "     * 计算右邻熵\n",
    "     */\n",
    "    public float computeRightEntropy(String prefix) {\n",
    "        Set<Map.Entry<String, Integer>> entrySet = trieRight.prefixSearch(prefix);\n",
    "        return computeEntropy(entrySet, prefix);\n",
    "    }\n",
    "\n",
    "\n",
    "    /**\n",
    "     * 信息熵计算\n",
    "     */\n",
    "    private float computeEntropy(Set<Map.Entry<String, Integer>> entrySet, String prefix) {\n",
    "        float totalFrequency = 0;\n",
    "        for (Map.Entry<String, Integer> entry : entrySet) {\n",
    "            if (entry.getKey().length() != prefix.length() + 1) {\n",
    "                continue;\n",
    "            }\n",
    "            totalFrequency += entry.getValue();\n",
    "        }\n",
    "        float le = 0;\n",
    "        for (Map.Entry<String, Integer> entry : entrySet) {\n",
    "            if (entry.getKey().length() != prefix.length() + 1) {\n",
    "                continue;\n",
    "            }\n",
    "            float p = entry.getValue() / totalFrequency;\n",
    "            le += -p * Math.log(p);\n",
    "        }\n",
    "        return le;\n",
    "    }\n",
    "```\n",
    "\n",
    "##### 4. 将候选词串以同一位置词开头划分为若干组。每组进行第一轮筛选，具体的办法为每个词进行归一化打分并逆序排序，最终召回得分最高的。在筛选的过程中进行互信息和信息熵阈值的过滤，所以第一轮筛选也可能没有候选词召回。\n",
    "\n",
    "如候选短句：**未来五年大众汽车将向北美市场投资**\n",
    "分为如下若干组：\n",
    ">[[未来, 未来五, 未来五年, 未来五年大],\n",
    " [来五, 来五年, 来五年大, 来五年大众], \n",
    "[五年, 五年大, 五年大众, 五年大众汽],\n",
    " [年大, 年大众, 年大众汽, 年大众汽车],\n",
    " [大众, 大众汽, 大众汽车, 大众汽车将],\n",
    " [众汽, 众汽车, 众汽车将, 众汽车将向],\n",
    " [汽车, 汽车将, 汽车将向, 汽车将向北], \n",
    "[车将, 车将向, 车将向北, 车将向北美], \n",
    "[将向, 将向北, 将向北美, 将向北美市],\n",
    " [向北, 向北美, 向北美市, 向北美市场],\n",
    " [北美, 北美市, 北美市场, 北美市场投], \n",
    "[美市, 美市场, 美市场投, 美市场投资],\n",
    " [市场, 市场投, 市场投资],\n",
    " [场投, 场投资],\n",
    " [投资]]\n",
    "##### 5. 对第一轮筛选召回的结果进行第二轮筛选，具体办法与第一轮类似，通过对候选词逆序排序，并过滤掉候选词之间有位置重叠关系的，最终得到抽词结果。\n",
    "\n",
    "##### 6. 如果在此基础上需要做分词，将原句以抽词结果间隔开，我们认为剩下的也很大可能是词。分词效果在开源人民日报数据上评测其分词准确率得到了还算不错的效果。不过更好的做法是把这个只用来做新词发现，然后补充用户词典，辅助词典分词。\n",
    "\n",
    "\n",
    "![抽取人民日报词语部分结果](https://upload-images.jianshu.io/upload_images/4941834-b01d96714980cfa9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "\n",
    "![抽取人民日报词语部分结果](https://upload-images.jianshu.io/upload_images/4941834-af9945ad5bf7ef10.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "\n",
    "\n",
    "![人民日报数据上抽词的效果](https://upload-images.jianshu.io/upload_images/4941834-fa860020e16bc8ab.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "\n",
    "![人民日报数据上抽词的效果](https://upload-images.jianshu.io/upload_images/4941834-2ad1c5297b1bd890.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "\n",
    "\n",
    "\n",
    "## 4. 大数据量的存储和计算问题\n",
    "\n",
    "大数据量的存储和计算问题，目前已经有成熟的大数据存储与计算开源方案来解决这类问题。但是有的实际情况下没有条件整那么多分布式集群机器，那可不可以在单机的情况下也解决一部分这样的问题，或者是缓解一些存储和计算的压力呢？答案是肯定的，一般单机的计算，在数据量和计算能力过剩的情况下，一般数据直接存内存，计算上也不要过多考虑。但是当内存空间存在瓶颈，这个时候就考虑采用外部存储的方式，但是又可以权衡io的时间消耗问题。为解决这部分问题，我采用了redis作为这样的外部存储系统，将计算的中间结果存在了redis中，利用了redis取数据常数时间复杂度的特性。存储的问题解决了，计算的问题一般是通过分治法的思想去做，一般我能够想到也可以有两种，一种是多进程，通过多个进程去分发任务，最后汇总计算结果。另一种是在一个进程里通过多线程的方式去加速计算，多线程的情况下并非越多线程越好，线程之间的切换带来了cpu的开销，但是在线程数量合理的情况下也是有一定的效果的。\n",
    "\n",
    "Java语言多线程并发计算可以采用 线程池ThreadPoolExecutor+阻塞队列+栅栏CountDownLatch+原子类AtomicInteger这样的方案来做。基本的思路是把阻塞队列当成消息队列来用，把全量的数据生产进阻塞队列，通过多线程去队列里消费领取自己的计算任务，通过栅栏进行线程之间的协调和通信，等到所有线程执行完毕，然后汇总计算结果。\n",
    "\n",
    "\n",
    "代码实现开源在 github：https://github.com/shanjunwei/SegAndNewWordDiscover\n",
    "\n",
    "参考：\n",
    "\n",
    "[互联网时代的社会语言学：基于SNS的文本数据挖掘](http://www.matrix67.com/blog/archives/5044)\n",
    "\n",
    "[一种没有语料字典的分词方法](https://blog.csdn.net/ygrx/article/details/8926274)\n",
    "\n",
    "[互信息公式及概述](http://www.omegaxyz.com/2018/08/02/mi/)\n",
    " [Aho Corasick自动机结合DoubleArrayTrie极速多模式匹配](http://www.hankcs.com/program/algorithm/aho-corasick-double-array-trie.html)\n",
    "\n",
    "[信息熵（香农熵）概述](http://www.omegaxyz.com/2018/05/07/information_entropy/)\n",
    "\n",
    "[基于信息熵的无字典分词算法](https://blog.csdn.net/zxh19800626/article/details/50190803)\n",
    "\n",
    "[Java开源项目cws_evaluation：中文分词器分词效果评估对比](https://github.com/shanjunwei/cws_evaluation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
